# PPO algorithm configuration
# These map directly to opensimrl.algorithms.ppo.ppo(...) arguments.

seed: 0

# Training schedule
steps_per_epoch: 4000
epochs: 50
max_ep_len: 1000
save_freq: 10

# Discounting / advantages
gamma: 0.99
lam: 0.97

# PPO hyperparameters
clip_ratio: 0.2
target_kl: 0.01
pi_lr: 3e-4
vf_lr: 1e-3
train_pi_iters: 80
train_v_iters: 80

# Actor-critic architecture kwargs
ac_kwargs:
  hidden_sizes: [64, 64]

# Optional behavior of the trainer
return_history: true
